# George AI Project

you should start developing on it using VSCode devcontainers.

To start the george-web app you need to

- re-open in dev container
- create .env files in the root, `apps/chat-web`, `apps/georgeai-server` and `packages/pothos-graphql` following `env.example` files
- `pnpm dev`

Enjoy.

# Architecture

```mermaid
flowchart TD


  content -- Document uploaded/updated/deleted--> workflow
  workflow -- Embedding success/failed --> content
  docEmbedder -- Document processed--> workflow
  content -- Doc changed --> docEmbedder

  subgraph georgeFrontend[George Frontend ğŸ’»]
    chatbot[Chatbot ğŸ¤–]
    docGenerator[Output Doc ğŸ—ºï¸]
    georgeAdmin[George Admin]
  end

  subgraph otherFrontend[Custom Frontend]
    formProvider[Some Forms UI]
    mapProvider[Some Maps UI]
  end

  subgraph backend[George Backend]
    subgraph georgeAPI [George API]
      restApi[Rest]
      graphqlAPI[GraphQL]
      graphqlAdminAPI[GraphQLAdmin]
    end

    webSearch[(webSearch ğŸ—ƒï¸)]
    vectorStore[(vectorStore ğŸ—ƒï¸)]

    subgraph llmService[LLM Service ğŸ› ï¸]
      docEmbedder[docEmbedder ğŸ“„]
      contextChains[contextChains ğŸ”—]
    end

    georgeAPI --> contextChains


    docEmbedder -- write docs with embeddings --> vectorStore
    contextChains <-- similaritySearch --> vectorStore
    contextChains <-- webSearch --> webSearch
    subgraph model[LLM Model RunnerğŸ¤–]
      openAI[openAI]
      mistral[Mistral]
      xai[XAI]
      pineCone[PineCone]
    end

  end
  subgraph content[Content]
      pocketbase[Pocketbase ğŸ“¦]
      strapi[Strapi ğŸ“¦]
    end

  subgraph workflow[Workflow]
    camunda[Camunda]
    windmill[windmill]

  end

  contextChains <-- generate text --> model
  georgeFrontend <-- query with history session --> graphqlAPI
  otherFrontend <-- query with history session --> graphqlAPI
  georgeAdmin <-- configure --> graphqlAdminAPI
```

## Components

- **Pocketbase** ğŸ“¦
  - used by the publisher
  - used for uploading PDFs
  - stores PDFs locally
  - it will inform the LLM Service about the uploaded PDFs
- **Pocketbase Database** ğŸ—„ï¸
  - stores Pocketbase data using sqlite
- **LLM Service** ğŸ› ï¸
  - on backend service
  - consists of three components: GraphQL Endpoint, PDF Processor, Chains
- **GraphQL Endpoint** ğŸŒ
  - communication endpoint of the LLM Service
- **PDF Processor** ğŸ“„
  - processes the uploaded PDFs
  - extracts the text and embeddings
  - writes the extracted data and the embedding to the LLM Database
  - informs Pocketbase that the PDF has been processed
- **Chains** ğŸ”—
  - uses the embeddings in LLM Database as a retriever
  - contains the chains for chatbot and travel planner
- **LLM Database** ğŸ—ƒï¸
  - stores the extracted data and embeddings
  - must be database with vector search support
- **Frontend** ğŸ’»
  - one Frontend App with two routes: Chatbot and Travel Planner
- **Chatbot** ğŸ¤–
  - bot to chat about the PDFs
- **Travel Planner** ğŸ—ºï¸
  - to create travel plans based on the PDFs
